\documentclass[12pt]{article}

% Подключение пакетов
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\geometry{left=2cm, right=2cm, top=2cm, bottom=2cm}

\begin{document}

% Титульный лист
\begin{titlepage}
    \begin{center}
        \large
        \textbf{ННГУ им. Лобачевского / ИИТММ / ПМИ}\\[0.5cm]

        \vspace{4cm}
        \textbf{\Large Отчёт по выполнению задания}\\
        \textbf{\large <<Поиск кратчайших путей из одной вершины (алгоритм Дейкстры) с CRS хранением графов с использованием MPI>>}\\[3cm]

        \vspace{3cm}
        \textbf{Выполнил:}\\
        студент группы 3822Б1ПМоп3 \\
        \textit{Лаганина Елена Артемовна}\\[1cm]

        \textbf{Преподаватель:}\\
        \textit{Сысоев Александр Владимирович, доцент}\\[2cm]

        \vfill
        \textbf{Нижний Новгород, 2024 г.}
    \end{center}
\end{titlepage}

% Введение
\section*{Введение}
\addcontentsline{toc}{section}{Введение}
Алгоритм Дейкстры является одним из классических методов поиска кратчайших путей в графах с неотрицательными весами рёбер. Он широко используется в задачах маршрутизации, логистике, телекоммуникациях и других приложениях, где требуется найти оптимальный путь между двумя вершинами графа. Однако при работе с большими графами, такими как транспортные сети городов или социальные сети, традиционные последовательные реализации алгоритма могут оказаться неэффективными из-за высоких требований к вычислительным ресурсам и времени выполнения.

Для ускорения вычислений и увеличения масштабируемости алгоритмов применяются подходы параллельного программирования, такие как Message Passing Interface (MPI). Библиотека MPI предоставляет мощные инструменты для организации обмена данными между процессорами, позволяя эффективно распределять нагрузку и обрабатывать большие объёмы информации. В данной задаче рассматривается применение алгоритма Дейкстры для CRS-графа (Compressed Row Storage), который представляет собой компактный способ хранения разреженных матриц, часто используемый в научных расчётах и инженерных приложениях.
\newpage

% Постановка задачи
\section*{Постановка задачи}
\addcontentsline{toc}{section}{Постановка задачи}
\begin{itemize}
    \item \textbf{Цель работы:}
        \begin{itemize}
            \item Реализация алгоритма Дейкстры для CRS графа.
        \end{itemize}
    \item \textbf{Формат представления графа:}
        \begin{itemize}
           \item В формате CRS граф хранится с помощью трех массивов: массив со смежными вершинами для каждой, массив указателей на начало ребер в массиве смежных вершин и массив с весами ребер
        \end{itemize}
    \item \textbf{Инструменты разработки:}
        \begin{itemize}
             \item Использование библиотеки Boost.MPI для параллельной реализации.
        \end{itemize}
    \item \textbf{Сравнение производительности:}
         \begin{itemize}
            \item Проведение сравнительного анализа производительности.
            \item Сравнение последовательной реализации с параллельной.
         \end{itemize}
\end{itemize}
\newpage

% Описание алгоритма
\section*{Алгоритм построения выпуклой оболочки для компонент бинарного изображения}
\addcontentsline{toc}{section}{Алгоритм построения выпуклой оболочки для компонент бинарного изображения}

\begin{itemize}
    \item[] \textbf{Входные данные:} Матрица, смежности в CRS формате.
    \item[] \textbf{Выходные данные:} Вектор, состоящий из наименьших расстояний до вершины.
\end{itemize}

\textbf{Алгоритм:}

\begin{enumerate}
    \item \textbf{Инициализация:}
        \begin{itemize}
            \item Создать вектор, проинициализированный максимальной целочисленной константой, размерности равной количесву вершин в графе.
            \item Задать матрицу смежности графа, представить ее в CRS(Compressed Sparse Rows) формате.
        \end{itemize}
    \item \textbf{Основной цикл (пока не все вершины посещены):}
        \begin{itemize}
            \item Найти непосещенную вершину u с минимальным расстоянием:
                \begin{itemize}
                    \item Перебрать все вершины, и выбрать вершину u с наименьшим значением distances[u], при условии, что visited[u] == 0.
                    \item Если все вершины посещены, завершить алгоритм.
                \end{itemize}
            \item Пометить вершину u как посещенную: Установить visited[u] = 1.
            \item Обновление расстояний до соседних вершин v вершины u:
                \begin{itemize}
                    \item айти диапазон соседних вершин для u в массиве индексов столбцов. Для этого:
                        \begin{itemize}
                            \item Найти индекс начала списка соседей для вершины u в массиве указателей.
                            \item Найти индекс конца списка соседей для вершины u в массиве указателей (либо использовать длину списка, либо индекс начала списка соседей следующей вершины).
                        \end{itemize}
                    \item Для каждой соседней вершины v в этом диапазоне:
                        \begin{itemize}
                            \item Найти вес ребра между вершиной u и v используя тот же индекс, что и в массиве индексов столбцов.
                            \item Вычислить расстояние до v через u: altdist = distances[u] + вес ребра от u к v.
                            \item Если altdist < distances[v] (т.е., нашли более короткий путь до v): * Обновить расстояние distances[v] = altdist.
                        \end{itemize}
                  \end{itemize}
             \end{itemize}
    \item \textbf{Завершение:}                          
        \begin{itemize}
            \item После завершения цикла массив distances будет содержать кратчайшие расстояния от начальной вершины src до всех остальных вершин графа.
        \end{itemize}

\end{enumerate}
\newpage

% Описание схемы распараллеливания
\section*{Описание схемы распараллеливания}
\addcontentsline{toc}{section}{Описание схемы распараллеливания}
Для повышения производительности используется параллельное программирование на основе Boost.MPI.

\subsection*{Основные отличия от последовательной версии}
В представленной параллельной реализации алгоритма Дейкстры для повышения вычислительной эффективности используется распределенная обработка графа между несколькими процессами. Суть подхода заключается в том, что на начальном этапе определяются все непосредственные потомки стартовой вершины, которые затем распределяются между вычислительными процессами. Каждый процесс, получив потомка, вычисляет кратчайшие расстояния от этого потомка до всех остальных вершин графа параллельно. При этом используется локальный массив расстояний, и глубина обхода графа в каждом процессе будет меньше по сравнению с последовательным алгоритмом. За счет параллельной обработки, вычислительная нагрузка на каждом процессе снижается пропорционально числу задействованных процессов. После завершения вычислений на каждом процессе, результаты собираются на управляющем процессе. На управляющем процессе также вычисляются расстояния от стартовой вершины до всех ее потомков и добавляются к полученным расстояниям. Далее происходит сбор минимальных расстояний, с использованием операции reduce, для получения окончательных значений. Полученные минимальные расстояния сравниваются с локальными данными на управляющем процессе и обновляются, таким образом, формируется финальный массив кратчайших расстояний от стартовой вершины до всех вершин графа. Основным преимуществом данного алгоритма является распараллеливание вычислений, что обеспечивает сокращение общего времени выполнения.
\subsection*{Механизмы параллелизма}
В данной реализации используются  2 механизма - это broadcast/reduce.

\subsection*{Завершение}
Программа завершается, когда корневой процесс завершает вычисления и помещает результат в структуру данных TaskData, для вывода. Результат вычислений совпадает с результатом последовательной версии алгоритма.

\newpage

\subsection*{Тесты производительности}
\label{subsec:performance_tests}

\begin{itemize}
    \item Операционная система: Windows 10
    \item Процессор: Intel Core i5-13400 (10 ядер x 3,5 ГГц)
    \item Оперативная память: 16 ГБ.
\end{itemize}

Эксперименты проводились на различном количестве процессов для оценки производительности параллельной реализации построения выпуклой оболочки для компонент бинарного изображения. Были измерены времена выполнения: \textit{pipeline\_run} и \textit{task\_run}.

\begin{itemize}
    \item \textbf{1 процесс:}
        \begin{itemize}
            \item \textit{pipeline\_run}: 7.2665438000
            \item \textit{task\_run}:  7.3214361000
        \end{itemize}
    \item \textbf{2 процесса:}
        \begin{itemize}
            \item \textit{pipeline\_run}:3.7001863000
            \item \textit{task\_run}: 3.7189029000
        \end{itemize}
     \item \textbf{3 процесса:}
        \begin{itemize}
            \item \textit{pipeline\_run}: 2.573325400
             \item \textit{task\_run}:  2.6811451000
        \end{itemize}
     \item \textbf{4 процесса:}
        \begin{itemize}
             \item \textit{pipeline\_run}: 2.0566192000
            \item \textit{task\_run}: 2.1110899000
        \end{itemize}
\end{itemize}

\subsection*{Подтверждение корректности}
\label{subsec:correctness_verification}
Корректность разработанного алгоритма подтверждается посредством сравнения результатов, полученных параллельной реализацией с результатами, полученными последовательной реализацией на тех же входных данных. Данные результаты были сравнены, и их полная идентичность говорит о правильности работы алгоритма построения выпуклой оболочки для компонент бинарного изображения.


% Заключение
\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}
Корректность разработанного параллельного алгоритма была подтверждена путём сравнения его выходных данных с результатами, полученными от последовательной версии алгоритма. Идентичность результатов, вычисленных на одинаковых входных данных, свидетельствует о правильной работе разработанного параллельного алгоритма поиска наикратчайшего пути(алгоритма Дейкстры) для CRS графа.
\newpage

% Приложение
\section*{Приложение}
\addcontentsline{toc}{section}{Приложение}
Ниже приведены ключевые фрагменты кода реализации:

%-------------------------------------------------------------------
\subsection*{Файл \texttt{ops\_seq.hpp}}
\addcontentsline{toc}{subsection}{Файл ops\_seq.hpp}
\begin{verbatim}
#pragma once

#include <vector>

#include "core/task/include/task.hpp"

namespace laganina_e_sum_values_by_columns_matrix_seq {

std::vector<int> getRandomVector(int sz);
class sum_values_by_columns_matrix_Seq : public ppc::core::Task {
 public:
  explicit sum_values_by_columns_matrix_Seq(std::shared_ptr<ppc::core::TaskData> 
  taskData_): Task(std::move(taskData_)) {}
  bool pre_processing() override;
  bool validation() override;
  bool run() override;
  bool post_processing() override;

 private:
  std::vector<int> input_;
  std::vector<int> res_;
  int m{};
  int n{};
};

}  // namespace laganina_e_sum_values_by_columns_matrix_seq
\end{verbatim}
\newpage

%-------------------------------------------------------------------
\subsection*{Файл \texttt{ops\_sec.cpp}}
\addcontentsline{toc}{subsection}{Файл ops\_sec.cpp}
\begin{verbatim}
#include "seq/laganina_e_sum_values_by_columns_matrix/include/ops_seq.hpp"

#include <thread>
#include <vector>

bool laganina_e_sum_values_by_columns_matrix_seq::sum_values_by_columns_matrix_Seq::
pre_processing() {
  internal_order_test();
  input_ = std::vector<int>(taskData->inputs_count[0]);
  m = taskData->inputs_count[1];
  n = taskData->inputs_count[2];
  auto* ptr = reinterpret_cast<int*>(taskData->inputs[0]);
  for (unsigned int i = 0; i < taskData->inputs_count[0]; i++) {
    input_[i] = ptr[i];
  }
  res_ = std::vector<int>(n, 0);
  return true;
}

bool laganina_e_sum_values_by_columns_matrix_seq::sum_values_by_columns_matrix_Seq::
validation() {
  internal_order_test();
  if (taskData->inputs_count[2] != taskData->outputs_count[0]) {
    return false;
  };
  if (taskData->inputs_count[1] < 1 || taskData->inputs_count[2] < 1) {
    return false;
  }
  if (taskData->inputs_count[0] != taskData->inputs_count[1] * taskData->inputs_count[2]) 
  {
    return false;
  }
  return true;
}

bool laganina_e_sum_values_by_columns_matrix_seq::sum_values_by_columns_matrix_Seq::run() 
{
  internal_order_test();
  for (int j = 0; j < n; j++) {
    int sum = 0;
    for (int i = 0; i < m; i++) {
      sum += input_[i * n + j];
    }
    res_[j] = sum;
  }
  return true;
}

bool laganina_e_sum_values_by_columns_matrix_seq::sum_values_by_columns_matrix_Seq::
post_processing() {
  internal_order_test();
  for (int i = 0; i < n; i++) {
    reinterpret_cast<int*>(taskData->outputs[0])[i] = res_[i];
  }
  return true;
}

\end{verbatim}
%-------------------------------------------------------------------
\subsection*{Файл \texttt{ops\_mpi.hpp}}

\begin{verbatim}
#pragma once

#include <gtest/gtest.h>
#include <random>

#include <boost/mpi/collectives.hpp>
#include <boost/mpi/communicator.hpp>
#include <vector>

#include "core/task/include/task.hpp"

namespace laganina_e_dejskras_a_mpi {

std::vector<int> getRandomgraph(int v);
int minDistanceVertex(const std::vector<int>& dist, const std::vector<int>& marker);

class TestMPITaskSequential : public ppc::core::Task {
 public:
  explicit TestMPITaskSequential(std::shared_ptr<ppc::core::TaskData> 
  taskData_):Task(std::move(taskData_)) {}
  bool pre_processing() override;
  bool validation() override;
  bool run() override;
  bool post_processing() override;
  static void dijkstra(int start_vertex, const std::vector<int>& row_ptr, const 
  std::vector<int>& col_ind,const std::vector<int>& data, int v, std::vector<int>& 
  distances);
  static void get_children_with_weights(int vertex, const std::vector<int>& row_ptr, 
  const std::vector<int>& col_ind,const std::vector<int>& data, std::vector<int>& 
  neighbor,td::vector<int>& weight);

 private:
  std::vector<int> row_ptr;
  std::vector<int> col_ind;
  std::vector<int> data;
  int v{};  // dimension
  std::vector<int> distances;
};
class TestMPITaskParallel : public ppc::core::Task {
 public:
  explicit TestMPITaskParallel(std::shared_ptr<ppc::core::TaskData> taskData_) : 
  Task(std::move(taskData_)) {}
  bool pre_processing() override;
  bool validation() override;
  bool run() override;
  bool post_processing() override;

 private:
  std::vector<int> row_ptr;
  std::vector<int> col_ind;
  std::vector<int> data;
  int v{};  // dimension
  std::vector<int> distances;

  boost::mpi::communicator world;
};

}  // namespace laganina_e_dejskras_a_mpi
\end{verbatim}

%-------------------------------------------------------------------
\subsection*{Файл \texttt{ops\_mpi.cpp}}

\begin{verbatim}
#include "mpi/laganina_e_dejkstras_a/include/ops_mpi.hpp"
#include <boost/mpi/communicator.hpp>

#include <vector>
#include <queue>

int laganina_e_dejskras_a_mpi::minDistanceVertex(const std::vector<int>& dist, 
const std::vector<int>& marker) {
  int minvalue = INT_MAX;
  int res = -1;
  for (int i = 0; i < static_cast<int>(dist.size()); ++i) {
    if (marker[i] == 0 && dist[i] <= minvalue) {
      minvalue = dist[i];
      res = i;
    }
  }
  return res;
}

bool laganina_e_dejskras_a_mpi::TestMPITaskSequential::pre_processing() {
  internal_order_test();
  v = taskData->inputs_count[0];
  int* ptr = reinterpret_cast<int*>(taskData->inputs[0]);

  int* matrix_row = new int[v * v];
  for (int i = 0; i < v * v; i++) {
    matrix_row[i] = ptr[i];
  }

  int num_edges = 0;
  for (int i = 0; i < v * v; i++) {
    if (matrix_row[i] != 0) {
      num_edges++;
    }
  }
  row_ptr.resize(v + 1, 0);
  col_ind.resize(num_edges);
  data.resize(num_edges);
  int edge_index = 0;
  for (int i = 0; i < v; i++) {
    row_ptr[i] = edge_index;
    for (int j = 0; j < v; j++) {
      if (matrix_row[i * v + j] != 0) {
        col_ind[edge_index] = j;
        data[edge_index] = matrix_row[i * v + j];
        edge_index++;
      }
    }
  }
  row_ptr[v] = edge_index;
  return true;
}

bool laganina_e_dejskras_a_mpi::TestMPITaskSequential::validation() {
  internal_order_test();
  if (taskData->inputs_count[0] <= 0) {
    return false;
  }
  return true;
}

bool laganina_e_dejskras_a_mpi::TestMPITaskSequential::run() {
  internal_order_test();
  laganina_e_dejskras_a_mpi::TestMPITaskSequential::dijkstra(0, row_ptr, col_ind, data,v 
  distances);
  return true;
}

bool laganina_e_dejskras_a_mpi::TestMPITaskSequential::post_processing() {
  internal_order_test();
  for (int i = 0; i < v; i++) {
    reinterpret_cast<int*>(taskData->outputs[0])[i] = distances[i];
  }
  return true;
}

bool laganina_e_dejskras_a_mpi::TestMPITaskParallel::pre_processing() {
  internal_order_test();
  return true;
}

bool laganina_e_dejskras_a_mpi::TestMPITaskParallel::validation() {
  internal_order_test();
  if (world.rank() == 0) {
    if (taskData->inputs_count[0] <= 0) {
      return false;
    }
  }
  return true;
}

bool laganina_e_dejskras_a_mpi::TestMPITaskParallel::run() {
  internal_order_test();

  int num_edges;

  if (world.rank() == 0) {
    v = taskData->inputs_count[0];
    int* ptr = reinterpret_cast<int*>(taskData->inputs[0]);

    int* matrix_row = new int[v * v];
    for (int i = 0; i < v * v; i++) {
      matrix_row[i] = ptr[i];
    }

    num_edges = 0;
    for (int i = 0; i < v * v; i++) {
      if (matrix_row[i] != 0) {
        num_edges++;
      }
    }
    row_ptr.resize(v + 1, 0);
    col_ind.resize(num_edges);
    data.resize(num_edges);
    int edge_index = 0;
    for (int i = 0; i < v; i++) {
      row_ptr[i] = edge_index;
      for (int j = 0; j < v; j++) {
        if (matrix_row[i * v + j] != 0) {
          col_ind[edge_index] = j;
          data[edge_index] = matrix_row[i * v + j];
          edge_index++;
        }
      }
    }
    row_ptr[v] = edge_index;
    delete[] matrix_row;
  }

  boost::mpi::broadcast(world, num_edges, 0);
  boost::mpi::broadcast(world, v, 0);
  distances.resize(v, INT_MAX);
  row_ptr.resize(v + 1, 0);
  col_ind.resize(num_edges);
  data.resize(num_edges);
  std::vector<int> res;
  std::vector<int> neighbor;
  std::vector<int> weight;
  std::vector<int> local_neighbor;
  std::vector<int> local_weight;
  int max_rank;
  int delta;
  int last;
  int size;

  boost::mpi::broadcast(world, row_ptr.data(), static_cast<int>(row_ptr.size()), 0);
  boost::mpi::broadcast(world, col_ind.data(), static_cast<int>(col_ind.size()), 0);
  boost::mpi::broadcast(world, data.data(), static_cast<int>(data.size()), 0);

  if (world.rank() == 0) {
    laganina_e_dejskras_a_mpi::TestMPITaskSequential::get_children_with_weights(0, 
    row_ptr, col_ind, data,neighbor, weight);
    if (world.size() >= static_cast<int>(neighbor.size())) {
      max_rank = static_cast<int>(neighbor.size()) - 1;
      delta = 1;
      last = 1;
    } else {
      max_rank = world.size() - 1;
      delta = static_cast<int>(neighbor.size()) / world.size();
      last = delta + (static_cast<int>(neighbor.size()) % world.size());
    }
    size = std::max({last, delta});
  }

  boost::mpi::broadcast(world, max_rank, 0);
  boost::mpi::broadcast(world, delta, 0);
  boost::mpi::broadcast(world, last, 0);
  boost::mpi::broadcast(world, size, 0);

  if (world.rank() == 0) {
    int rank = 1;
    local_neighbor.resize(delta);
    std::copy(neighbor.begin(), neighbor.begin() + delta, local_neighbor.begin());
    local_weight.resize(delta);
    std::copy(weight.begin(), weight.begin() + delta, local_weight.begin());
    for (int i = delta; rank <= max_rank; i += delta) {
      if (rank == max_rank) {
        world.send(rank, 0, neighbor.data() + i, last);
        world.send(rank, 0, weight.data() + i, last);
        rank++;
        break;
      }
      world.send(rank, 0, neighbor.data() + i, delta);
      world.send(rank, 0, weight.data() + i, delta);
      rank++;
    }
  } 
  else {
    if (world.rank() == max_rank) {
      local_neighbor.resize(last);
      local_weight.resize(last);
      world.recv(0, 0, local_neighbor.data(), last);
      world.recv(0, 0, local_weight.data(), last);
    } else if (world.rank() < max_rank) {
      local_neighbor.resize(delta);
      local_weight.resize(delta);
      world.recv(0, 0, local_neighbor.data(), delta);
      world.recv(0, 0, local_weight.data(), delta);
    }
  }

  if (world.rank() == 0) {
    distances[0] = 0;
  }
  for (int k = 0; k < size; k++) {
    std::vector<int> tmp;
    if (world.rank() == 0) {
      tmp = distances;
    }
    res.resize(v, INT_MAX);
    if ((!local_neighbor.empty()) && (world.rank() <= max_rank)) {
      int vertex = local_neighbor.back();
      local_neighbor.pop_back();
      int bonus = local_weight.back();
      local_weight.pop_back();
      laganina_e_dejskras_a_mpi::TestMPITaskSequential::dijkstra(vertex, row_ptr, 
      col_ind, data, v, res);
      for (int& t : res) {
        if (t != INT_MAX) {
          if ((t < 0) || (bonus == INT_MAX))
            t = INT_MAX;
          else
            t += bonus;
        }
      }
    }
   
    boost::mpi::reduce(world, res.data(), v, distances.data(), boost::mpi::minimum<int>(), 
    0);
    if (world.rank() == 0) {
      for (int i = 0; i < v; i++) {
        distances[i] = std::min({distances[i], tmp[i]});
      }
    }
  }

  return true;
}

bool laganina_e_dejskras_a_mpi::TestMPITaskParallel::post_processing() {
  internal_order_test();
  if (world.rank() == 0) {
    for (int i = 0; i < v; i++) {
      reinterpret_cast<int*>(taskData->outputs[0])[i] = distances[i];
    }
  }
  return true;
}

void laganina_e_dejskras_a_mpi::TestMPITaskSequential::get_children_with_weights(
    int vertex, const std::vector<int>& row_ptr, const std::vector<int>& col_ind, 
    const std::vector<int>& data,
    std::vector<int>& neighbor, std::vector<int>& weight) {
  // Get the beginning and end of edges for a given vertex
  int start = row_ptr[vertex];
  int end = row_ptr[vertex + 1];

  for (int i = start; i < end; ++i) {
    neighbor.push_back(col_ind[i]);  // Neighboring vertex
    weight.push_back(data[i]);       // Edge weight
  }
}

void laganina_e_dejskras_a_mpi::TestMPITaskSequential::dijkstra(int start_vertex, 
const std::vector<int>& row_ptr,
const std::vector<int>& col_ind,const std::vector<int>& data, int v,
std::vector<int>& distances) {
  // Initialize distances
  distances.resize(v, std::numeric_limits<int>::max());
  distances[start_vertex] = 0;

  // Array to track visited vertices
  std::vector<bool> visited(v, false);

  // Priority queue for storing pairs (distance, vertex)
  std::priority_queue<std::pair<int, int>, std::vector<std::pair<int, int>>, 
  std::greater<std::pair<int, int>>>
      priority_queue;
  priority_queue.emplace(0, start_vertex);  // Use start_vertex instead of 0

  while (!priority_queue.empty()) {
    int current_distance = priority_queue.top().first;
    int current_vertex = priority_queue.top().second;
    priority_queue.pop();

    // If the vertex has already been visited, skip it
    if (visited[current_vertex]) {
      continue;
    }

    // Mark the vertex as visited
    visited[current_vertex] = true;

    // Process all neighboring vertices
    int start = row_ptr[current_vertex];
    int end = row_ptr[current_vertex + 1];
    for (int i = start; i < end; ++i) {
      int neighbor_vertex = col_ind[i];
      int weight = data[i];
      int new_distance = current_distance + weight;

      // If a shorter distance is found, update it
      if (new_distance < distances[neighbor_vertex]) {
        distances[neighbor_vertex] = new_distance;
        priority_queue.emplace(new_distance, neighbor_vertex);
      }
    }
  }
}
\end{verbatim}

\end{document}