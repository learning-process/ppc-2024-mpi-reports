\documentclass[12pt]{article}

% Подключение пакетов
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\geometry{left=2cm, right=2cm, top=2cm, bottom=2cm}

\begin{document}

% Титульный лист
\begin{titlepage}
    \begin{center}
        \large
        \textbf{ННГУ им. Лобачевского / ИИТММ / ПМИ}\\[0.5cm]

        \vspace{4cm}
        \textbf{\Large Отчёт по выполнению задания}\\
        \textbf{\large <<Умножение разреженных матриц в формате CCS с использованием MPI>>}\\[3cm]

        \vspace{3cm}
        \textbf{Выполнил:}\\
        студент группы 3822Б1ПМоп3 \\
        \textit{Шляков Максим Сергеевич}\\[1cm]

        \textbf{Преподаватель:}\\
        \textit{Сысоев Александр Владимирович, доцент}\\[2cm]

        \vfill
        \textbf{Нижний Новгород, 2024 г.}
    \end{center}
\end{titlepage}

% Введение
\section*{Введение}
\addcontentsline{toc}{section}{Введение}
Задача умножения разреженных матриц в формате CCS имеет важное значение в различных областях, включая научные расчёты и обработку больших данных. Для повышения эффективности вычислений используются методы параллельной обработки данных с использованием библиотеки Boost.MPI, обеспечивающей высокую гибкость и производительность.
\newpage

% Постановка задачи
\section*{Постановка задачи}
\addcontentsline{toc}{section}{Постановка задачи}
\begin{itemize}
    \item \textbf{Цель работы:}
        \begin{itemize}
            \item Разработка программы для умножения разреженных матриц.
        \end{itemize}
    \item \textbf{Формат представления матриц:}
        \begin{itemize}
            \item Использование формата CCS (Compressed Column Storage).
        \end{itemize}
    \item \textbf{Инструменты разработки:}
        \begin{itemize}
             \item Использование библиотеки Boost.MPI для параллельной реализации.
        \end{itemize}
    \item \textbf{Сравнение производительности:}
         \begin{itemize}
            \item Проведение сравнительного анализа производительности.
            \item Сравнение последовательной реализации с параллельной.
         \end{itemize}
\end{itemize}
\newpage

% Описание алгоритма
\section*{Алгоритм умножения разреженных матриц в формате CCS (последовательная реализация)}
\addcontentsline{toc}{section}{Алгоритм умножения разреженных матриц в формате CCS (последовательная реализация)}
\textbf{Входные данные:} Две разреженные матрицы, представленные в формате CCS. Для каждой матрицы это:
\begin{itemize}
    \item Массив значений ненулевых элементов (values).
    \item Массив индексов строк, соответствующих значениям (row indices).
    \item Массив указателей на начало каждого столбца в массивах значений и индексов строк (column pointers).
\end{itemize}

\textbf{Выходные данные:} Результирующая разреженная матрица в формате CCS.

\textbf{Алгоритм:}

\begin{enumerate}
    \item \textbf{Инициализация:}
        \begin{itemize}
            \item Создать пустые массивы для хранения результирующей матрицы (значения, индексы строк, указатели столбцов).
            \item В массив указателей столбцов добавить 0 (начало первого столбца).
            \item Создать и инициализировать нулями временный массив, размер которого равен количеству строк первой матрицы. Этот массив будет использоваться для хранения промежуточных результатов при вычислении каждого столбца результирующей матрицы.
        \end{itemize}

    \item \textbf{Перебор столбцов второй матрицы:}
        \begin{itemize}
            \item Для каждого столбца второй матрицы:
            \begin{itemize}
                \item Обнулить все элементы временного массива.
            \end{itemize}
        \end{itemize}

    \item \textbf{Вычисление элементов столбца результирующей матрицы:}
        \begin{itemize}
            \item Для каждого ненулевого элемента в текущем столбце второй матрицы:
                \begin{itemize}
                    \item  Получить значение и индекс строки элемента.
                    \item Использовать индекс строки как индекс столбца в первой матрице.
                    \item Для каждого ненулевого элемента в этом столбце первой матрицы:
                        \begin{itemize}
                            \item Получить значение и индекс строки элемента.
                            \item Умножить значения текущих элементов из первой и второй матриц.
                            \item  Прибавить полученное произведение к элементу временного массива, индекс которого равен индексу строки элемента первой матрицы.
                        \end{itemize}
                \end{itemize}
         \end{itemize}

    \item \textbf{Сохранение результатов для текущего столбца:}
        \begin{itemize}
            \item Для каждой строки первой матрицы:
                \begin{itemize}
                   \item Если значение во временном массиве для текущей строки не равно нулю:
                    \begin{itemize}
                        \item Добавить это значение в массив значений результирующей матрицы.
                        \item Добавить индекс текущей строки в массив индексов строк результирующей матрицы.
                    \end{itemize}
                 \end{itemize}
        \end{itemize}

    \item \textbf{Обновление указателей столбцов:}
    \begin{itemize}
        \item  В массив указателей столбцов результирующей матрицы добавить текущую длину массива значений результирующей матрицы.
    \end{itemize}
    
    \item \textbf{Завершение:}
       \begin{itemize}
           \item Повторять шаги 2-5 для всех столбцов второй матрицы.
           \item Результатом будет являться результирующая матрица, представленная в формате CCS.
       \end{itemize}

\end{enumerate}
\newpage

% Описание схемы распараллеливания
\section*{Описание схемы распараллеливания}
\addcontentsline{toc}{section}{Описание схемы распараллеливания}
Для повышения производительности используется библиотека Boost.MPI. Основные этапы распараллеливания:

\subsection*{1. Деление задачи}
Для распараллеливания задачи умножения разреженных матриц использован подход деления на подзадачи. Входная матрица \( B \) делится по столбцам между процессами. Каждому процессу назначается определённое количество столбцов \( cols\_per\_proc \), причём остаток \( remainder \) распределяется между первыми \( remainder \) процессами. Это позволяет сбалансировать нагрузку на процессы.

\begin{itemize}
    \item Процессы с рангом \( rank < remainder \) получают \( cols\_per\_proc + 1 \) столбцов.
    \item Остальные процессы получают \( cols\_per\_proc \) столбцов.
\end{itemize}

\subsection*{2. Передача данных}
Перед началом вычислений данные матриц \( A \) и \( B \) передаются от ведущего процесса (с рангом 0) ко всем остальным процессам с использованием функции \texttt{boost::mpi::broadcast}. Передаваемые данные включают массивы значений (\texttt{values}), индексов строк (\texttt{row\_indices}) и указателей на столбцы (\texttt{col\_pointers}).

\subsection*{3. Локальные вычисления}
Каждый процесс вычисляет часть результирующей матрицы, ограниченную назначенными ему столбцами \( start\_col \) и \( end\_col \). Этапы локальных вычислений:
\begin{enumerate}
    \item Создание подматрицы \( B\_subset \), включающей столбцы \( B \) в диапазоне от \( start\_col \) до \( end\_col \).
    \item Для каждого столбца в \( B\_subset \):
    \begin{itemize}
        \item Вычисляются промежуточные значения для строк матрицы \( C \) с использованием временного массива \( Cj\_map \).
        \item Промежуточные значения фильтруются (исключаются значения меньше порога \( 1e{-12} \)) и добавляются в массивы результирующей подматрицы \( C\_subset \).
    \end{itemize}
\end{enumerate}

\subsection*{4. Сборка результатов}
После завершения локальных вычислений все процессы отправляют свои подматрицы \( C\_subset \) ведущему процессу с использованием функции \texttt{boost::mpi::gather}. Ведущий процесс объединяет части результирующей матрицы \( C \), добавляя значения, индексы строк и указатели столбцов в массивы результирующей матрицы.

\subsection*{5. Завершение}
Процесс завершения включает:
\begin{itemize}
    \item Резервирование памяти в массиве результирующей матрицы для оптимизации вставки данных.
    \item Объединение данных всех процессов в матрицу \( C \), представленную в формате CCS.
\end{itemize}

\textbf{Результат:} Матрица \( C \), вычисленная параллельно, совпадает с результатом последовательного алгоритма, но с существенно меньшими временными затратами благодаря распараллеливанию.
\newpage

\subsection*{Тесты производительности}
\label{subsec:performance_tests}

\begin{itemize}
    \item Операционная система: Windows 10
    \item Процессор: Intel Core i7-9700F (8 ядер x 3ГГц)
    \item Оперативная память: 32 ГБ.
\end{itemize}

Эксперименты проводились на различном количестве процессов для оценки производительности параллельной реализации умножения разреженных матриц. Были измерены времена выполнения: \textit{pipeline\_run} и \textit{task\_run}.

\begin{itemize}
    \item \textbf{1 процесс:}
        \begin{itemize}
            \item \textit{pipeline\_run}: 6.7185724000
            \item \textit{task\_run}:  6.7870091000
        \end{itemize}
    \item \textbf{2 процесса:}
        \begin{itemize}
            \item \textit{pipeline\_run}: 4.9082842001
            \item \textit{task\_run}: 4.9716545001
        \end{itemize}
     \item \textbf{3 процесса:}
        \begin{itemize}
            \item \textit{pipeline\_run}: 4.3697611000
             \item \textit{task\_run}: 4.3609425002
        \end{itemize}
     \item \textbf{4 процесса:}
        \begin{itemize}
             \item \textit{pipeline\_run}: 3.9976907000
            \item \textit{task\_run}: 3.9527797999
        \end{itemize}
\end{itemize}

\subsection*{Подтверждение корректности}
\label{subsec:correctness_verification}
Корректность разработанного алгоритма подтверждается посредством сравнения результатов, полученных параллельной реализацией с результатами, полученными последовательной реализацией на тех же входных данных. Данные результаты были сравнены, и их полная идентичность говорит о правильности работы алгоритма умножения разреженных матриц.


% Заключение
\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}
Разработанная программа использует библиотеку Boost.MPI для реализации параллельного умножения разреженных матриц. Результаты экспериментов показывают, что время выполнения алгоритма уменьшается с увеличением числа процессов, что свидетельствует об эффективности распараллеливания.
\newpage

% Приложение
\section*{Приложение}
\addcontentsline{toc}{section}{Приложение}
Ниже приведены ключевые фрагменты кода реализации:

%-------------------------------------------------------------------
\subsection*{Файл \texttt{ops\_seq.hpp}}
\addcontentsline{toc}{subsection}{Файл ops\_seq.hpp}
\begin{verbatim}
#pragma once

#include <string>
#include <vector>

#include "core/task/include/task.hpp"

namespace shlyakov_m_ccs_mult {

struct SparseMatrix {
  std::vector<double> values;
  std::vector<int> row_indices;
  std::vector<int> col_pointers;
};

class TestTaskSequential : public ppc::core::Task {
 public:
  explicit TestTaskSequential(std::shared_ptr<ppc::core::TaskData> taskData_) 
    : Task(std::move(taskData_)) {}
  
  bool pre_processing() override;
  bool validation() override;
  bool run() override;
  bool post_processing() override;

 private:
  SparseMatrix A_;
  int rows_a;
  int cols_a;
  SparseMatrix B_;
  int rows_b;
  int cols_b;
  SparseMatrix result_;
};

}  // namespace shlyakov_m_ccs_mult
\end{verbatim}
\newpage

%-------------------------------------------------------------------
\subsection*{Файл \texttt{ops\_sec.cpp}}
\addcontentsline{toc}{subsection}{Файл ops\_sec.cpp}
\begin{verbatim}
// ops_sec.cpp
// Реализация последовательного умножения матриц в формате CCS

#include "seq/shlyakov_m_ccs_mult/include/ops_seq.hpp"

#include <algorithm>
#include <thread>
#include <vector>

using namespace shlyakov_m_ccs_mult;

bool TestTaskSequential::pre_processing() {
  internal_order_test();

  const auto* a_values = reinterpret_cast<const double*>(taskData->inputs[0]);
  const auto* a_row_indices = reinterpret_cast<const int*>(taskData->inputs[1]);
  const auto* a_col_pointers = reinterpret_cast<const int*>(taskData->inputs[2]);

  const auto* b_values = reinterpret_cast<const double*>(taskData->inputs[3]);
  const auto* b_row_indices = reinterpret_cast<const int*>(taskData->inputs[4]);
  const auto* b_col_pointers = reinterpret_cast<const int*>(taskData->inputs[5]);

  A_.values.assign(a_values, a_values + taskData->inputs_count[0]);
  A_.row_indices.assign(a_row_indices, a_row_indices + taskData->inputs_count[1]);
  A_.col_pointers.assign(a_col_pointers, a_col_pointers + taskData->inputs_count[2] + 1);

  B_.values.assign(b_values, b_values + taskData->inputs_count[3]);
  B_.row_indices.assign(b_row_indices, b_row_indices + taskData->inputs_count[4]);
  B_.col_pointers.assign(b_col_pointers, b_col_pointers + taskData->inputs_count[5] + 1);

  rows_a = taskData->inputs_count[2];
  rows_b = taskData->inputs_count[5];
  cols_a = static_cast<int>(A_.col_pointers.size()) - 1;
  cols_b = static_cast<int>(B_.col_pointers.size()) - 1;

  return true;
}

bool TestTaskSequential::validation() {
  internal_order_test();

  bool is_invalid = taskData == nullptr || taskData->inputs.size() != 6 || taskData->inputs_count.size() < 6 ||
                    static_cast<int>(taskData->inputs_count[2]) < 0 ||
                    static_cast<int>(taskData->inputs_count[5]) < 0 ||
                    static_cast<int>(taskData->inputs_count[0]) != static_cast<int>(taskData->inputs_count[1]) ||
                    static_cast<int>(taskData->inputs_count[3]) != static_cast<int>(taskData->inputs_count[4]) ||
                    (taskData->inputs_count[0] <= 0 && taskData->inputs_count[3] <= 0);

  return !is_invalid;
}

bool TestTaskSequential::run() {
  internal_order_test();

  result_.col_pointers.clear();
  result_.values.clear();
  result_.row_indices.clear();
  result_.col_pointers.push_back(0);

  std::vector<double> temp;
  temp.resize(rows_a, 0.0);

  int k;
  int pos_a;
  int row_a;
  int b_start;
  int b_end;
  int pos_b;
  double a_val;
  double b_val;

  for(int col_b = 0; col_b < cols_b; ++col_b) {
    std::fill(temp.begin(), temp.end(), 0.0);
    b_start = B_.col_pointers[col_b];
    b_end = B_.col_pointers[col_b + 1];
    for (int pos_b = b_start; pos_b < b_end; ++pos_b) {
      b_val = B_.values[pos_b];
      k = B_.row_indices[pos_b];
      int a_start = A_.col_pointers[k];
      int a_end = A_.col_pointers[k + 1];
      for (pos_a = a_start; pos_a < a_end; ++pos_a) {
        a_val = A_.values[pos_a];
        row_a = A_.row_indices[pos_a];
        temp[row_a] += a_val * b_val;
      }
    }
    result_.col_pointers.push_back(static_cast<int>(result_.values.size()));
    for (row_a = 0; row_a < rows_a; ++row_a) {
      if (temp[row_a] != 0.0) {
        result_.values.push_back(temp[row_a]);
        result_.row_indices.push_back(row_a);
      }
    }
    result_.col_pointers.push_back(static_cast<int>(result_.values.size()));
  }

  return true;
}

bool TestTaskSequential::post_processing() {
  internal_order_test();

  taskData->outputs.push_back(reinterpret_cast<uint8_t*>(result_.values.data()));
  taskData->outputs_count.push_back(static_cast<unsigned int>(result_.values.size() * sizeof(double)));

  taskData->outputs.push_back(reinterpret_cast<uint8_t*>(result_.row_indices.data()));
  taskData->outputs_count.push_back(static_cast<unsigned int>(result_.row_indices.size() * sizeof(int)));

  taskData->outputs.push_back(reinterpret_cast<uint8_t*>(result_.col_pointers.data()));
  taskData->outputs_count.push_back(static_cast<unsigned int>(result_.col_pointers.size() * sizeof(int)));

  return true;
}


}  // namespace shlyakov_m_ccs_mult
\end{lstlisting}
\end{verbatim}
%-------------------------------------------------------------------
\subsection*{Файл \texttt{ops\_mpi.hpp}}

\begin{verbatim}
// ops_mpi.hpp
// Реализация параллельного умножения матриц в формате CCS с использованием MPI
#pragma once

#include <boost/mpi/collectives.hpp>
#include <boost/mpi/communicator.hpp>
#include <boost/serialization/nvp.hpp>
#include <boost/serialization/serialization.hpp>
#include <boost/serialization/vector.hpp>
#include <cmath>
#include <map>
#include <numeric>
#include <unordered_set>
#include <vector>

#include "core/task/include/task.hpp"

namespace shlyakov_m_ccs_mult_mpi {

struct SparseMatrix {
  std::vector<double> values;
  std::vector<int> row_indices;
  std::vector<int> col_pointers;

  template <class Archive>
  void serialize(Archive& ar, const unsigned int version) {
    ar & values;
    ar & row_indices;
    ar & col_pointers;
  }
};

class TestTaskMPI : public ppc::core::Task {
 public:
  explicit TestTaskMPI(std::shared_ptr<ppc::core::TaskData> taskData_) 
    : Task(std::move(taskData_)), world() {}
  
  bool pre_processing() override;
  bool validation() override;
  bool run() override;
  bool post_processing() override;

 private:
  SparseMatrix A_;
  int rows_a;
  int cols_a;
  SparseMatrix B_;
  int rows_b;
  int cols_b;
  SparseMatrix result_;

  boost::mpi::communicator world;
};

}  // namespace shlyakov_m_ccs_mult_mpi
\end{verbatim}

%-------------------------------------------------------------------
\subsection*{Файл \texttt{ops\_mpi.cpp}}

\begin{verbatim}
// ops_mpi.cpp
// Реализация методов класса TestTaskMPI для параллельного умножения матриц

#include "mpi/shlyakov_m_ccs_mult_mpi/include/ops_mpi.hpp"

#include <thread>
#include <vector>

using namespace shlyakov_m_ccs_mult_mpi;

bool TestTaskMPI::pre_processing() {
  internal_order_test();

  if (world.rank() == 0) {
    const auto* a_values = reinterpret_cast<const double*>(taskData->inputs[0]);
    const auto* a_row_indices = reinterpret_cast<const int*>(taskData->inputs[1]);
    const auto* a_col_pointers = reinterpret_cast<const int*>(taskData->inputs[2]);

    const auto* b_values = reinterpret_cast<const double*>(taskData->inputs[3]);
    const auto* b_row_indices = reinterpret_cast<const int*>(taskData->inputs[4]);
    const auto* b_col_pointers = reinterpret_cast<const int*>(taskData->inputs[5]);

    A_.values.assign(a_values, a_values + taskData->inputs_count[0]);
    A_.row_indices.assign(a_row_indices, a_row_indices + taskData->inputs_count[1]);
    A_.col_pointers.assign(a_col_pointers, a_col_pointers + taskData->inputs_count[2] + 1);

    B_.values.assign(b_values, b_values + taskData->inputs_count[3]);
    B_.row_indices.assign(b_row_indices, b_row_indices + taskData->inputs_count[4]);
    B_.col_pointers.assign(b_col_pointers, b_col_pointers + taskData->inputs_count[5] + 1);

    rows_a = taskData->inputs_count[2];
    rows_b = taskData->inputs_count[5];
    cols_a = static_cast<int>(A_.col_pointers.size()) - 1;
    cols_b = static_cast<int>(B_.col_pointers.size()) - 1;
  }

  return true;
}

bool TestTaskMPI::validation() {
  internal_order_test();

  if (world.rank() == 0) {
    if (taskData == nullptr || taskData->inputs.size() != 6 || taskData->inputs_count.size() < 6 ||
        static_cast<int>(taskData->inputs_count[2]) < 0 || static_cast<int>(taskData->inputs_count[5]) < 0 ||
        static_cast<int>(taskData->inputs_count[0]) != static_cast<int>(taskData->inputs_count[1]) ||
        static_cast<int>(taskData->inputs_count[3]) != static_cast<int>(taskData->inputs_count[4]) ||
        (taskData->inputs_count[0] <= 0 && taskData->inputs_count[3] <= 0)) {
      return false;
    }
  }

  return true;
}

bool TestTaskMPI::run() {
  internal_order_test();

  // Рассылка данных матриц всем процессам
  boost::mpi::broadcast(world, A_.values, 0);
  boost::mpi::broadcast(world, A_.row_indices, 0);
  boost::mpi::broadcast(world, A_.col_pointers, 0);
  boost::mpi::broadcast(world, rows_a, 0);
  boost::mpi::broadcast(world, cols_a, 0);

  boost::mpi::broadcast(world, B_.values, 0);
  boost::mpi::broadcast(world, B_.row_indices, 0);
  boost::mpi::broadcast(world, B_.col_pointers, 0);
  boost::mpi::broadcast(world, rows_b, 0);
  boost::mpi::broadcast(world, cols_b, 0);

  // Определение диапазона столбцов для текущего процесса
  int cols_per_proc = cols_b / world.size();
  int remainder = cols_b % world.size();

  int start_col;
  int end_col;
  if (world.rank() < remainder) {
    start_col = world.rank() * (cols_per_proc + 1);
    end_col = start_col + cols_per_proc + 1;
  } else {
    start_col = world.rank() * cols_per_proc + remainder;
    end_col = start_col + cols_per_proc;
  }

  SparseMatrix C_subset;
  C_subset.col_pointers.reserve(end_col - start_col + 1);
  C_subset.col_pointers.push_back(0);

  std::vector<double> temp(rows_a, 0.0);
  std::vector<double> local_values;
  std::vector<int> local_row_indices;
  std::vector<int> local_col_ptr(end_col - start_col + 1, 0);

  int local_nnz_count = 0;

  // Локальное умножение столбцов
  for (int j = start_col; j < end_col; ++j) {
    std::fill(temp.begin(), temp.end(), 0.0);

    for (int posA = A_.col_pointers[j]; posA < A_.col_pointers[j + 1]; ++posA) {
      double a_val = A_.values[posA];
      int a_row = A_.row_indices[posA];
      for (int posB = B_.col_pointers[a_row]; posB < B_.col_pointers[a_row + 1]; ++posB) {
        int b_row = B_.row_indices[posB];
        double b_val = B_.values[posB];
        temp[b_row] += a_val * b_val;
      }
    }

    C_subset.col_pointers[j - start_col] = local_nnz_count;
    for (int row = 0; row < rows_a; ++row) {
      if (std::abs(temp[row]) > 1e-12) {
        local_values.push_back(temp[row]);
        local_row_indices.push_back(row);
        ++local_nnz_count;
      }
    }
    C_subset.col_pointers[j - start_col + 1] = local_nnz_count;
  }

  // Сбор всех частичных результатов на корневом процессе
  std::vector<SparseMatrix> all_C_subsets;
  if (world.rank() == 0) {
    all_C_subsets.resize(world.size());
  }

  boost::mpi::gather(world, C_subset, all_C_subsets, 0);

  if (world.rank() == 0) {
    result_.values.reserve(cols_b * rows_a);
    result_.row_indices.reserve(cols_b * rows_a);
    result_.col_pointers.reserve(cols_b + 1);
    result_.col_pointers.push_back(0);

    for (int proc = 0; proc < world.size(); ++proc) {
      SparseMatrix& C_proc = all_C_subsets[proc];
      int num_cols = C_proc.col_pointers.size() - 1;

      for (int j = 0; j < num_cols; ++j) {
        int col_start = C_proc.col_pointers[j];
        int col_end = C_proc.col_pointers[j + 1];

        result_.values.insert(result_.values.end(), C_proc.values.begin() + col_start, C_proc.values.begin() + col_end);
        result_.row_indices.insert(result_.row_indices.end(), C_proc.row_indices.begin() + col_start,
                                   C_proc.row_indices.begin() + col_end);

        result_.col_pointers.push_back(static_cast<int>(result_.values.size()));
      }
    }
  }

  return true;
}

bool TestTaskMPI::post_processing() {
  internal_order_test();

  if (world.rank() == 0) {
    taskData->outputs.push_back(reinterpret_cast<uint8_t*>(result_.values.data()));
    taskData->outputs_count.push_back(static_cast<unsigned int>(result_.values.size() * sizeof(double)));

    taskData->outputs.push_back(reinterpret_cast<uint8_t*>(result_.row_indices.data()));
    taskData->outputs_count.push_back(static_cast<unsigned int>(result_.row_indices.size() * sizeof(int)));

    taskData->outputs.push_back(reinterpret_cast<uint8_t*>(result_.col_pointers.data()));
    taskData->outputs_count.push_back(static_cast<unsigned int>(result_.col_pointers.size() * sizeof(int)));
  }

  return true;
}

}  // namespace shlyakov_m_ccs_mult_mpi
\end{verbatim}

\end{document}
